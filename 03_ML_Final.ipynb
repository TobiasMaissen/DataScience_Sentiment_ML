{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers datasets optuna scikit-learn matplotlib torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoConfig\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import optuna\n",
    "from transformers import TrainerCallback\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch Dataset class to handle tokenized inputs and their corresponding labels.\n",
    "\n",
    "    Attributes:\n",
    "        encodings (dict): Tokenized input data.\n",
    "        labels (list): List of labels corresponding to the input data.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a single data sample by index, including inputs and label.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the data sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing tokenized input tensors and label tensor.\n",
    "        \"\"\"\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class EarlyStoppingCallbackCustom(TrainerCallback):\n",
    "    def __init__(self, patience=2):\n",
    "        self.patience = patience\n",
    "        self.best_loss = None\n",
    "        self.epochs_no_improve = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics is None:\n",
    "            return\n",
    "\n",
    "        eval_loss = metrics.get(\"eval_loss\")\n",
    "        if eval_loss is None:\n",
    "            return\n",
    "\n",
    "        if self.best_loss is None or eval_loss < self.best_loss:\n",
    "            self.best_loss = eval_loss\n",
    "            self.epochs_no_improve = 0\n",
    "            control.should_save = True  # Speichere das Modell, wenn es besser ist\n",
    "        else:\n",
    "            self.epochs_no_improve += 1\n",
    "            if self.epochs_no_improve >= self.patience:\n",
    "                print(f\"Validation loss has not improved for {self.patience} evaluations. Stopping training.\")\n",
    "                control.should_training_stop = True\n",
    "\n",
    "\n",
    "# Custom TrainingArguments erstellen\n",
    "class CustomTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, optimizer=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "# Custom Trainer erstellen\n",
    "class CustomTrainer(Trainer):\n",
    "    def create_optimizer(self):\n",
    "        optimizer_name = self.args.optimizer\n",
    "        if optimizer_name == \"adamw\":\n",
    "            optimizer_cls = torch.optim.AdamW\n",
    "            optimizer_kwargs = {\n",
    "                \"lr\": self.args.learning_rate,\n",
    "                \"weight_decay\": self.args.weight_decay,\n",
    "            }\n",
    "        elif optimizer_name == \"adafactor\":\n",
    "            from transformers.optimization import Adafactor\n",
    "            optimizer_cls = Adafactor\n",
    "            optimizer_kwargs = {\n",
    "                \"lr\": self.args.learning_rate,\n",
    "                \"weight_decay\": self.args.weight_decay,\n",
    "                \"scale_parameter\": False,\n",
    "                \"relative_step\": False,\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "        self.optimizer = optimizer_cls(self.model.parameters(), **optimizer_kwargs)\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maissen:\n",
    "    def __init__(self, model_names, use_drive=False, hpo_n_trials=17):\n",
    "        self.model_names = model_names\n",
    "        self.data_path = \"/content/drive/MyDrive/MAS DataScience/CAS_ML/training_data.csv\" if use_drive else \"training_data.csv\"\n",
    "        self.save_base_dir = \"/content/drive/MyDrive/MAS DataScience/CAS_ML/saved_models\" if use_drive else \"./saved_models\"\n",
    "        self.use_drive = use_drive\n",
    "        self.hpo_n_trials = hpo_n_trials\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.training_args = None\n",
    "        self.trainer = None\n",
    "\n",
    "        # Conditionally mount Google Drive\n",
    "        if self.use_drive:\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "\n",
    "        # Adjust paths if using Google Drive\n",
    "        if self.use_drive:\n",
    "            self.data_path = \"/content/drive/MyDrive/MAS DataScience/CAS_ML/training_data.csv\"\n",
    "            self.save_base_dir = \"/content/drive/MyDrive/MAS DataScience/CAS_ML/saved_models\"\n",
    "        else:\n",
    "            self.data_path = \"training_data.csv\"\n",
    "            self.save_base_dir = \"./saved_models\"\n",
    "\n",
    "    def load_data(self):\n",
    "        df = pd.read_csv(self.data_path)\n",
    "        df = df[['relevant_sentence', 'label']]\n",
    "        label_map = {'negativ': 0, 'neutral': 1, 'positiv': 2}\n",
    "        df['label'] = df['label'].map(label_map)\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            df['relevant_sentence'].tolist(),\n",
    "            df['label'].tolist(),\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=df['label']\n",
    "        )\n",
    "        return train_texts, val_texts, train_labels, val_labels\n",
    "\n",
    "    def prepare_dataset(self, texts, labels):\n",
    "        encodings = self.tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
    "        return SentimentDataset(encodings, labels)\n",
    "\n",
    "    def model_init(self, model_name):\n",
    "        model_config = AutoConfig.from_pretrained(model_name, num_labels=3)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, config=model_config, ignore_mismatched_sizes=True\n",
    "        )\n",
    "        return self.model\n",
    "\n",
    "    def set_training_arguments(self, output_dir='./results'):  # Adjust arguments as needed\n",
    "        self.training_args = CustomTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            eval_strategy='epoch',\n",
    "            save_strategy='epoch',\n",
    "            logging_dir='./logs',\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model='eval_loss',\n",
    "            greater_is_better=False,\n",
    "            save_total_limit=1,\n",
    "            logging_steps=10\n",
    "        )\n",
    "\n",
    "    def perform_hyperparameter_search(self, model_name, train_dataset, val_dataset):\n",
    "        def optuna_hp_space(trial):\n",
    "            return {\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "                \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "                \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [2, 5, 10]),\n",
    "                \"weight_decay\": trial.suggest_categorical(\"weight_decay\", [0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]),\n",
    "                \"warmup_steps\": trial.suggest_int(\"warmup_steps\", 0, 300),\n",
    "                \"optimizer\": trial.suggest_categorical(\"optimizer\", [\"adamw\", \"adafactor\"]),\n",
    "            }\n",
    "\n",
    "        self.trainer = CustomTrainer(\n",
    "            model_init=lambda: self.model_init(model_name),\n",
    "            args=self.training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallbackCustom()],\n",
    "        )\n",
    "\n",
    "        best_run = self.trainer.hyperparameter_search(\n",
    "            direction=\"minimize\",\n",
    "            backend=\"optuna\",\n",
    "            hp_space=optuna_hp_space,\n",
    "            n_trials=self.hpo_n_trials,\n",
    "            compute_objective=lambda metrics: metrics[\"eval_loss\"],\n",
    "        )\n",
    "\n",
    "        # Set best hyperparameters\n",
    "        for n, v in best_run.hyperparameters.items():\n",
    "            setattr(self.trainer.args, n, v)\n",
    "\n",
    "        # Update optimizer using the best hyperparameters\n",
    "        self.trainer.create_optimizer()\n",
    "\n",
    "        return best_run\n",
    "\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, predictions, average='weighted', zero_division=1\n",
    "        )\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "    def train_model(self, train_dataset, val_dataset):\n",
    "        if not self.trainer:\n",
    "            self.trainer = CustomTrainer(\n",
    "                model=self.model,\n",
    "                args=self.training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=val_dataset,\n",
    "                compute_metrics=self.compute_metrics,\n",
    "                callbacks=[EarlyStoppingCallbackCustom()]\n",
    "            )\n",
    "        self.trainer.train()\n",
    "\n",
    "    def run(self):\n",
    "        train_texts, val_texts, train_labels, val_labels = self.load_data()\n",
    "        results_list = []\n",
    "        os.makedirs(self.save_base_dir, exist_ok=True)\n",
    "\n",
    "        for model_name in self.model_names:\n",
    "            print(f\"\\n===== Starte HPO für Modell: {model_name} =====\")\n",
    "\n",
    "            # Load tokenizer\n",
    "            try:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Laden des Tokenizers für {model_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Prepare datasets\n",
    "            try:\n",
    "                train_dataset = self.prepare_dataset(train_texts, train_labels)\n",
    "                val_dataset = self.prepare_dataset(val_texts, val_labels)\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Erstellen der Datensätze für {model_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Perform hyperparameter optimization\n",
    "            try:\n",
    "                self.set_training_arguments(output_dir=f'./results/{model_name.replace(\"/\", \"_\")}')\n",
    "                best_run = self.perform_hyperparameter_search(model_name, train_dataset, val_dataset)\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler bei der Hyperparameter-Optimierung für {model_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Save best parameters and model\n",
    "            result = {\n",
    "                'model_name': model_name,\n",
    "                'best_params': best_run.hyperparameters,\n",
    "                'best_accuracy': best_run.objective\n",
    "            }\n",
    "            results_list.append(result)\n",
    "\n",
    "            print(f\"Beste Hyperparameter für {model_name}: {best_run.hyperparameters}\")\n",
    "            print(f\"Beste Validierungsgenauigkeit: {best_run.objective:.4f}\")\n",
    "\n",
    "            model_save_path = os.path.join(self.save_base_dir, model_name.replace('/', '_'))\n",
    "            try:\n",
    "                self.trainer.save_model(model_save_path)\n",
    "                self.tokenizer.save_pretrained(model_save_path)\n",
    "                hyperparams_path = os.path.join(model_save_path, 'best_hyperparams.json')\n",
    "                with open(hyperparams_path, 'w') as f:\n",
    "                    json.dump({\n",
    "                        'best_params': best_run.hyperparameters,\n",
    "                        'best_accuracy': best_run.objective\n",
    "                    }, f, indent=4)\n",
    "                print(f\"Modell und Hyperparameter für {model_name} gespeichert unter: {model_save_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Speichern des Modells für {model_name}: {e}\")\n",
    "\n",
    "        # Save results summary\n",
    "        results_df = pd.DataFrame(results_list)\n",
    "        results_csv_path = os.path.join(self.save_base_dir, 'optuna_hpo_summary.csv')\n",
    "        results_df.to_csv(results_csv_path, index=False)\n",
    "        print(f\"Hyperparameter-Optimierungsergebnisse gespeichert unter: {results_csv_path}\")\n",
    "\n",
    "    def generate_learning_curves(self):\n",
    "        saved_models_dir = self.save_base_dir  # Use the directory where models are saved\n",
    "        train_sizes = np.linspace(0.1, 1.0, 5)  # Training sizes from 10% to 100%\n",
    "        learning_curves = {}\n",
    "\n",
    "        # List of saved model directories\n",
    "        model_dirs = [\n",
    "            os.path.join(saved_models_dir, d)\n",
    "            for d in os.listdir(saved_models_dir)\n",
    "            if os.path.isdir(os.path.join(saved_models_dir, d))\n",
    "        ]\n",
    "\n",
    "        # Load validation data\n",
    "        _, val_texts, _, val_labels = self.load_data()\n",
    "\n",
    "        for model_dir in model_dirs:\n",
    "            model_name = os.path.basename(model_dir).replace('_', '/')\n",
    "            print(f\"\\n===== Verarbeite Modell: {model_name} =====\")\n",
    "\n",
    "            # Load the best hyperparameters and accuracy\n",
    "            hyperparams_path = os.path.join(model_dir, 'best_hyperparams.json')\n",
    "            try:\n",
    "                with open(hyperparams_path, 'r') as f:\n",
    "                    hyperparams_data = json.load(f)\n",
    "                best_params = hyperparams_data['best_params']\n",
    "                best_accuracy = hyperparams_data['best_accuracy']\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Laden der Hyperparameter für {model_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Load tokenizer and model\n",
    "            try:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "                self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Laden des Modells oder Tokenizers für {model_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Prepare validation dataset\n",
    "            try:\n",
    "                val_dataset = self.prepare_dataset(val_texts, val_labels)\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler bei der Vorbereitung des Validierungsdatensatzes für {model_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Prepare training dataset\n",
    "            try:\n",
    "                train_texts, _, train_labels, _ = self.load_data()\n",
    "                train_encodings_full = self.tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "                full_train_dataset = SentimentDataset(train_encodings_full, train_labels)\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler bei der Tokenisierung der Trainingsdaten für {model_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Initialize learning curve data for the model\n",
    "            learning_curves[model_name] = {\n",
    "                'train_sizes': [],\n",
    "                'train_scores': [],\n",
    "                'val_scores': []\n",
    "            }\n",
    "\n",
    "            for size_fraction in train_sizes:\n",
    "                subset_size = int(len(full_train_dataset) * size_fraction)\n",
    "                if subset_size < 1:\n",
    "                    subset_size = 1  # Ensure at least one example is used\n",
    "\n",
    "                print(f\"  Trainingsgröße: {subset_size} ({size_fraction*100:.0f}%)\")\n",
    "\n",
    "                # Create a subset of the training dataset\n",
    "                indices = np.random.choice(len(full_train_dataset), subset_size, replace=False)\n",
    "                train_subset = torch.utils.data.Subset(full_train_dataset, indices)\n",
    "\n",
    "                # Define training arguments\n",
    "                training_args = CustomTrainingArguments(\n",
    "                    output_dir='./temp_trainer',\n",
    "                    eval_strategy='no',\n",
    "                    save_strategy='no',\n",
    "                    logging_dir='./logs',\n",
    "                    logging_steps=10,\n",
    "                    per_device_train_batch_size=best_params['per_device_train_batch_size'],\n",
    "                    learning_rate=best_params['learning_rate'],\n",
    "                    num_train_epochs=best_params['num_train_epochs'],\n",
    "                    weight_decay=best_params['weight_decay'],\n",
    "                    warmup_steps=best_params['warmup_steps'],\n",
    "                    disable_tqdm=True,  # Avoid excessive output\n",
    "                    optimizer=best_params['optimizer'],\n",
    "                )\n",
    "\n",
    "                # Define trainer\n",
    "                trainer = CustomTrainer(\n",
    "                    model=self.model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=train_subset,\n",
    "                    eval_dataset=val_dataset,\n",
    "                    compute_metrics=self.compute_metrics\n",
    "                )\n",
    "\n",
    "                # Train the model\n",
    "                try:\n",
    "                    trainer.train()\n",
    "                except Exception as e:\n",
    "                    print(f\"Fehler beim Training für {model_name} mit Größe {subset_size}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Evaluate on training subset\n",
    "                try:\n",
    "                    train_results = trainer.evaluate(eval_dataset=train_subset)\n",
    "                    train_acc = train_results.get('eval_accuracy', 0)\n",
    "                except Exception as e:\n",
    "                    print(f\"Fehler bei der Evaluierung des Trainingssets für {model_name}: {e}\")\n",
    "                    train_acc = 0\n",
    "\n",
    "                # Evaluate on validation set\n",
    "                try:\n",
    "                    val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "                    val_acc = val_results.get('eval_accuracy', 0)\n",
    "                except Exception as e:\n",
    "                    print(f\"Fehler bei der Evaluierung des Validierungssets für {model_name}: {e}\")\n",
    "                    val_acc = 0\n",
    "\n",
    "                # Store results\n",
    "                learning_curves[model_name]['train_sizes'].append(subset_size)\n",
    "                learning_curves[model_name]['train_scores'].append(train_acc)\n",
    "                learning_curves[model_name]['val_scores'].append(val_acc)\n",
    "\n",
    "                # Optionally: Clean up temporary training directories\n",
    "                try:\n",
    "                    import shutil\n",
    "                    shutil.rmtree('./temp_trainer')\n",
    "                    shutil.rmtree('./logs')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # Clear model from memory to conserve resources\n",
    "            del self.model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Plot learning curves for each model\n",
    "        for model_name, curves in learning_curves.items():\n",
    "            plt.figure(figsize=(8, 6))\n",
    "\n",
    "            train_sizes = curves['train_sizes']\n",
    "            train_scores = curves['train_scores']\n",
    "            val_scores = curves['val_scores']\n",
    "\n",
    "            plt.plot(train_sizes, train_scores, 'o-', label='Training Score')\n",
    "            plt.plot(train_sizes, val_scores, 's-', label='Validation Score')\n",
    "\n",
    "            plt.xlabel('Trainingsgröße')\n",
    "            plt.ylabel('Genauigkeit')\n",
    "            plt.title(f'Lernkurve für {model_name}')\n",
    "            plt.legend(loc='best')\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'deepset/gbert-base',\n",
    "    'aari1995/German_Sentiment',\n",
    "    'oliverguhr/german-sentiment-bert',\n",
    "    'lxyuan/distilbert-base-multilingual-cased-sentiments-student',\n",
    "    'nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "    'distilbert-base-german-cased',\n",
    "    'xlm-roberta-base',\n",
    "    'ssary/XLM-RoBERTa-German-sentiment'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maissen = Maissen(model_names=model_names, use_drive=False, hpo_n_trials=20)\n",
    "maissen.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maissen.generate_learning_curves()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
